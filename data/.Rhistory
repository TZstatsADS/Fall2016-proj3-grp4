rbind(rep(1,2),rep(2,3))
cbind(rep(1,2),rep(2,3))
c(rep(1,2),rep(2,3))
test_data
test_data_frequency_number<-test_data$count_num[2:row_length]
test_data_interval<-test_data$absolute_hour[2:row_length]-test_data$absolute_hour[1:row_length-1]
hour_vector_initial<-rep(test_data_interval[1],test_data_frequency_number[1])
length(test_data_frequency_number)
hour_vector<-rep(test_data_interval[1],test_data_frequency_number[1])
for(i in 2:length(test_data_frequency_number))
{
hour_vector_temp<-rep(test_data_interval[i],test_data_frequency_number[i])
hour_vector<-c(hour_vector,hour_vector_temp)
}
hist(hour_vector)
#clear global environment
rm(list=ls(all=TRUE))
library(data.table)
library(dplyr)
# Import filtered data
crime_data<-fread('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_1.csv')
for(i in 2:20)
{
input_data<-fread(paste('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_',
as.character(i),'.csv',sep=''))
crime_data<-rbind(crime_data,input_data)
}
#rename some column
names(crime_data)[names(crime_data)=="Occurrence Year"] <- "Occurrence_Year"
#to speed up, use the 2015 data only
crime_data_part<-crime_data %>% filter(Occurrence_Year==2015)
#create an absoulte_hour variable used for calculating hour difference
crime_data_part$absolute_hour<-apply(crime_data_part,1,
function(x) as.numeric(x[9])+24*as.numeric(x[24]))
crime_data_part_group<-crime_data_part %>% group_by(absolute_hour,Precinct) %>% summarise(count_num=n())
Precinct<-unique(crime_data_part_group$Precinct)
Precinct
test_data<-crime_data_part_group[crime_data_part_group$Precinct==7,]
test_data<-test_data[order(test_data$absolute_hour),]
row_length<-nrow(test_data)
test_data_frequency_number<-test_data$count_num[2:row_length]
test_data_interval<-test_data$absolute_hour[2:row_length]-test_data$absolute_hour[1:row_length-1]
hour_vector<-rep(test_data_interval[1],test_data_frequency_number[1])
for(i in 2:length(test_data_frequency_number))
{
hour_vector_temp<-rep(test_data_interval[i],test_data_frequency_number[i])
hour_vector<-c(hour_vector,hour_vector_temp)
}
hist(hour_vector)
help(hist)
hist(hour_vector,breaks=100)
hist(hour_vector,breaks=50)
install.packages("Renext")
gofExp.test(hour_vector)
library(Renext)
help(gofExp.test)
gofExp.test(hour_vector)
gofExp.test(hour_vector)
a<-gofExp.test(hour_vector)
a[3]
library(MASS)
fit1 <- fitdistr(hour_vector, "exponential")
ks.test(hour_vector, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
install.packages("vcd")
library(vcd)
help(ks.test)
ks.test(hour_vector, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
output<-ks.test(hour_vector, "pexp", fit1$estimate) # p-value > 0.05 -> distribution not refused
ex <- rexp(10000, rate = 1.85)
fit2 <- fitdistr(ex, "exponential")
output2<-ks.test(ex, "pexp", fit1$estimate)
help(ks.test)
c<-0
for(precinct_num in Precinct)
{
c=c+precinct_num
}
paste('test_data',as.character(Precinct[1]),sep='')
paste('test_data','_',as.character(Precinct[1]),sep='')
help(dataframe)
help(dataframe)
test_data
length(Precinct)
output
hour_vector_total<-c()
rm(list=ls(all=TRUE))
crime_data<-fread('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_1.csv')
for(i in 2:20)
{
input_data<-fread(paste('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_',
as.character(i),'.csv',sep=''))
crime_data<-rbind(crime_data,input_data)
}
#rename some column
names(crime_data)[names(crime_data)=="Occurrence Year"] <- "Occurrence_Year"
#to speed up, use the 2015 data only
crime_data_part<-crime_data %>% filter(Occurrence_Year==2015)
#create an absoulte_hour variable used for calculating hour difference
crime_data_part$absolute_hour<-apply(crime_data_part,1,
function(x) as.numeric(x[9])+24*as.numeric(x[24]))
#group by absolute hour and precinct
crime_data_part_group<-crime_data_part %>% group_by(absolute_hour,Precinct) %>% summarise(count_num=n())
#Get the unique precinct
Precinct<-unique(crime_data_part_group$Precinct)
rm(list=ls(all=TRUE))
library(data.table)
library(dplyr)
# Import filtered data
crime_data<-fread('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_1.csv')
for(i in 2:20)
{
input_data<-fread(paste('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_',
as.character(i),'.csv',sep=''))
crime_data<-rbind(crime_data,input_data)
}
#rename some column
names(crime_data)[names(crime_data)=="Occurrence Year"] <- "Occurrence_Year"
#to speed up, use the 2015 data only
crime_data_part<-crime_data %>% filter(Occurrence_Year==2015)
#create an absoulte_hour variable used for calculating hour difference
crime_data_part$absolute_hour<-apply(crime_data_part,1,
function(x) as.numeric(x[9])+24*as.numeric(x[24]))
#group by absolute hour and precinct
crime_data_part_group<-crime_data_part %>% group_by(absolute_hour,Precinct) %>% summarise(count_num=n())
#Get the unique precinct
Precinct<-unique(crime_data_part_group$Precinct)
# For loop to get the p value for every district
hour_vector_total<-c()
result<-matrix(data=NA,nrow=length(Precinct),ncol=3)
library(MASS)
library(vcd)
for(precinct_num in 1:length(Precinct))
{
#get the data in a specific precinct
data_by_precinct<-crime_data_part_group[crime_data_part_group$Precinct==Precinct[precinct_num],]
#order by time , ascending
data_by_precinct<-data_by_precinct[order(data_by_precinct$absolute_hour),]
#get the length
row_length<-nrow(data_by_precinct)
#get the frequency number
data_by_precinct_frequency_number<-data_by_precinct$count_num[2:row_length]
#calculate the time interval by hour
data_by_precinct_interval<-data_by_precinct$absolute_hour[2:row_length]-data_by_precinct$absolute_hour[1:row_length-1]
#combine the time interval and frequency number to get the full data
hour_vector<-rep(data_by_precinct_interval[1],data_by_precinct_frequency_number[1])
for(i in 2:length(data_by_precinct_frequency_number))
{
hour_vector_temp<-rep(data_by_precinct_interval[i],data_by_precinct_frequency_number[i])
hour_vector<-c(hour_vector,hour_vector_temp)
}
#test if it;s exponential distribution
# estimate the parameters
parameters <- fitdistr(hour_vector, "exponential")
# goodness of fit test
output<-ks.test(hour_vector, "pexp", parameters$estimate) # p-value > 0.05 -> distribution not refused
result[precinct_num,1]<-Precinct[precinct_num]
result[precinct_num,2]<-output$p.value
result[precinct_num,3]<-length(hour_vector)
hour_vector_total<-c(hour_vector_total,hour_vector)
}
View(result)
View(result)
#test if it;s exponential distribution
# estimate the parameters
parameters <- fitdistr(hour_vector_total, "exponential")
# goodness of fit test
output<-ks.test(hour_vector_total, "pexp", parameters$estimate) # p-value > 0.05 -> distribution not refused
hist(hour_vector_total)
hist(hour_vector_total,breaks=50)
hist(hour_vector_total,breaks=100)
result
result[25,]
result[44,]
rm(list=ls(all=TRUE))
#clear global environment
rm(list=ls(all=TRUE))
library(data.table)
library(dplyr)
# Import filtered data
crime_data<-fread('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_1.csv')
for(i in 2:20)
{
input_data<-fread(paste('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/data/crime_data_',
as.character(i),'.csv',sep=''))
crime_data<-rbind(crime_data,input_data)
}
#rename some column
names(crime_data)[names(crime_data)=="Occurrence Year"] <- "Occurrence_Year"
#to speed up, use the 2015 data only
crime_data_part<-crime_data %>% filter(Occurrence_Year>=2014)
#create an absoulte_hour variable used for calculating hour difference
crime_data_part$absolute_hour<-apply(crime_data_part,1,
function(x) as.numeric(x[9])+24*as.numeric(x[24]))
#group by absolute hour and precinct
crime_data_part_group<-crime_data_part %>% group_by(absolute_hour,Precinct) %>% summarise(count_num=n())
#Get the unique precinct
Precinct<-unique(crime_data_part_group$Precinct)
# For loop to get the p value for every district
hour_vector_total<-c()
result<-matrix(data=NA,nrow=length(Precinct),ncol=3)
library(MASS)
library(vcd)
for(precinct_num in 1:length(Precinct))
{
#get the data in a specific precinct
data_by_precinct<-crime_data_part_group[crime_data_part_group$Precinct==Precinct[precinct_num],]
#order by time , ascending
data_by_precinct<-data_by_precinct[order(data_by_precinct$absolute_hour),]
#get the length
row_length<-nrow(data_by_precinct)
#get the frequency number
data_by_precinct_frequency_number<-data_by_precinct$count_num[2:row_length]
#calculate the time interval by hour
data_by_precinct_interval<-data_by_precinct$absolute_hour[2:row_length]-data_by_precinct$absolute_hour[1:row_length-1]
#combine the time interval and frequency number to get the full data
hour_vector<-rep(data_by_precinct_interval[1],data_by_precinct_frequency_number[1])
for(i in 2:length(data_by_precinct_frequency_number))
{
hour_vector_temp<-rep(data_by_precinct_interval[i],data_by_precinct_frequency_number[i])
hour_vector<-c(hour_vector,hour_vector_temp)
}
#test if it;s exponential distribution
# estimate the parameters
parameters <- fitdistr(hour_vector, "exponential")
# goodness of fit test
output<-ks.test(hour_vector, "pexp", parameters$estimate) # p-value > 0.05 -> distribution not refused
result[precinct_num,1]<-Precinct[precinct_num]
result[precinct_num,2]<-output$p.value
result[precinct_num,3]<-length(hour_vector)
hour_vector_total<-c(hour_vector_total,hour_vector)
}
result
#test if it;s exponential distribution
# estimate the parameters
parameters <- fitdistr(hour_vector_total, "exponential")
# goodness of fit test
output<-ks.test(hour_vector_total, "pexp", parameters$estimate) # p-value > 0.05 -> distribution not refused
hist(hour_vector_total,breaks=100)
hist(hour_vector_total, freq = FALSE, breaks = 100, xlim = c(0, quantile(hour_vector_total, 0.99)))
hist(hour_vector_total, freq = FALSE, breaks = 1000, xlim = c(0, quantile(hour_vector_total, 0.99)))
curve(dexp(x, rate = parameters$estimate), col = "red", add = TRUE)
View(crime_data_part)
View(crime_data_part)
View(crime_data_part)
View(crime_data_part)
crime_data_part_group_different<-crime_data_part %>% group_by(absolute_hour,Precinct,Offense) %>% summarise(count_num=n())
View(crime_data_part_group_different)
View(crime_data_part_group_different)
crime_data_part_group<-crime_data_part %>% group_by(absolute_hour,Precinct,Offense) %>% summarise(count_num=n())
View(crime_data_part)
View(crime_data_part)
# For loop to get the p value for every district
hour_vector_total<-c()
result<-matrix(data=NA,nrow=length(Precinct),ncol=3)
library(MASS)
library(vcd)
for(precinct_num in 1:length(Precinct))
{
#get the data in a specific precinct and a specific crime type
data_by_precinct<-crime_data_part_group[(crime_data_part_group$Precinct==Precinct[precinct_num]
&&crime_data_part_group$Offense=='GRAND LARCENY'),]
#order by time , ascending
data_by_precinct<-data_by_precinct[order(data_by_precinct$absolute_hour),]
#get the length
row_length<-nrow(data_by_precinct)
#get the frequency number
data_by_precinct_frequency_number<-data_by_precinct$count_num[2:row_length]
#calculate the time interval by hour
data_by_precinct_interval<-data_by_precinct$absolute_hour[2:row_length]-data_by_precinct$absolute_hour[1:row_length-1]
#combine the time interval and frequency number to get the full data
hour_vector<-rep(data_by_precinct_interval[1],data_by_precinct_frequency_number[1])
for(i in 2:length(data_by_precinct_frequency_number))
{
hour_vector_temp<-rep(data_by_precinct_interval[i],data_by_precinct_frequency_number[i])
hour_vector<-c(hour_vector,hour_vector_temp)
}
#test if it;s exponential distribution
# estimate the parameters
parameters <- fitdistr(hour_vector, "exponential")
# goodness of fit test
output<-ks.test(hour_vector, "pexp", parameters$estimate) # p-value > 0.05 -> distribution not refused
result[precinct_num,1]<-Precinct[precinct_num]
result[precinct_num,2]<-output$p.value
result[precinct_num,3]<-length(hour_vector)
hour_vector_total<-c(hour_vector_total,hour_vector)
}
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Github/Project2/Fall2016-Proj2-grp6/app')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test/test_app')
install.packages("devtools")
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test/test_app')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test/test_app')
load('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/Shiny_test/public_count.RData')
View(public_count)
View(public_count)
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test/test_app')
install.packages("shinydashboard")
library("shiny")
library("shinydashboard")
library("highcharter")
library("dplyr")
library("viridisLite")
library("markdown")
library("quantmod")
library("tidyr")
library("ggplot2")
library("treemap")
install.packages("treemap")
library("forecast")
library("DT")
install.packages("DT")
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test_1009/original')
library("DT")
install.packages("DT")
library("DT")
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test_yueqi')
data(mtcars)
data(mtcars)
mtcars
class(mtcars)
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
n<-200
K<-10
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold))
help(sample)
rm(list = ls())
pixel_feature<-readRDS('image_matrix.rds')
pixel_feature<-readRDS('image_matrix.rds')
rm(list = ls())
label[1:1000]<-0
train<-function(dataset_train){
library(e1071)
tune.out=tune(svm ,train.label~.,data=dataset_train ,kernel ='linear',
ranges =list(cost=c(0.01, 0.1,1)))
bestmod =tune.out$best.model
return(bestmod)
}
#####test
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4/data")
label<-rep(1,2000)
label[1:1000]<-0
#1.Using PCA processed SIFT features as feature, error rate around 33.1%.
#####train
train<-function(dataset_train){
library(e1071)
tune.out=tune(svm ,train.label~.,data=dataset_train ,kernel ='linear',
ranges =list(cost=c(0.01, 0.1,1)))
bestmod =tune.out$best.model
return(bestmod)
}
#####test
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
k_folds<-5
k<-100
set.seed(1)
s <- sample(rep(1:k_folds, c(rep(200, 10-1), 2000-(10-1)*200)))
s <- sample(rep(1:k_folds, c(rep(200, 10-1), 2000-(10-1)*200)))
k<-10
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3")
pixel_feature<-readRDS('image_matrix.rds')
k<-10
n <- nrow(pixel_feature)
n.fold <- floor(n/k)
set.seed(1)
s <- sample(rep(1:k, c(rep(n.fold, k-1), n-(k-1)*n.fold)))
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4/data")
label<-rep(1,2000)
label[1:1000]<-0
train<-function(dataset_train){
library(e1071)
tune.out=tune(svm ,train.label~.,data=dataset_train ,kernel ='linear',
ranges =list(cost=c(0.01, 0.1,1)))
bestmod =tune.out$best.model
return(bestmod)
}
#####test
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
k_folds<-5
#the pca features number
k<-100
set.seed(1)
s <- sample(rep(1:10, c(rep(200, 10-1), 2000-(10-1)*200)))
cv.error <- rep(NA, k_folds)
for(i in 1:k_folds)
{
load(paste('rgb_train_',as.character(i),'.RData',sep=''))
load(paste('rgb_test_',as.character(i),'.RData',sep=''))
train.data<-t(P_train[1:k,])
test.data<-t(P_test[1:k,])
train.label <- label[s != i]
test.label <- label[s == i]
train.data=data.frame(train.data,train.label=as.factor(train.label))
bestmod =train(train.data)
colnames(test.data)<-colnames(train.data)[1:k]
pred=test(bestmod ,test.data )
test.label=as.factor(test.label)
cv.error[i] <- mean(pred != test.label)
print(paste(as.character(i/k_folds*100),'%',' completed',sep=''))
}
result<-c(mean(cv.error),sd(cv.error))
#####train
train<-function(dataset_train){
library(randomForest)
fit_rf <- randomForest(train.label~.,data=dataset_train,type="classification",importance=TRUE)
return(fit_rf)
}
#####test
test<-function(fit_rf,dataset_test){
pred<-predict(fit_rf,newdata=dataset_test)
return(pred)
}
k_folds<-10
#the pca features number
k<-100
set.seed(1)
s <- sample(rep(1:10, c(rep(200, 10-1), 2000-(10-1)*200)))
cv.error <- rep(NA, k_folds)
for(i in 1:k_folds)
{
load(paste('rgb_train_',as.character(i),'.RData',sep=''))
load(paste('rgb_test_',as.character(i),'.RData',sep=''))
train.data<-t(P_train[1:k,])
test.data<-t(P_test[1:k,])
train.label <- label[s != i]
test.label <- label[s == i]
train.data=data.frame(train.data,train.label=as.factor(train.label))
fit <- train(train.data)
colnames(test.data)<-colnames(train.data)[1:k]
pred <- test(fit,test.data)
test.label=as.factor(test.label)
cv.error[i] <- mean(pred != test.label)
print(paste(as.character(i/k_folds*100),'%',' completed',sep=''))
}
result<-c(mean(cv.error),sd(cv.error))
k_folds<-10
#the pca features number
k<-100
set.seed(1)
s <- sample(rep(1:10, c(rep(200, 10-1), 2000-(10-1)*200)))
cv.error <- rep(NA, k_folds)
for(i in 1:k_folds)
{
load(paste('rgb_train_',as.character(i),'.RData',sep=''))
load(paste('rgb_test_',as.character(i),'.RData',sep=''))
train.data<-t(P_train[1:k,])
test.data<-t(P_test[1:k,])
train.label <- label[s != i]
test.label <- label[s == i]
train.data=data.frame(train.data,train.label=as.factor(train.label))
fit <- train(train.data)
colnames(test.data)<-colnames(train.data)[1:k]
pred <- test(fit,as.data.frame(test.data))
test.label=as.factor(test.label)
cv.error[i] <- mean(pred != test.label)
print(paste(as.character(i/k_folds*100),'%',' completed',sep=''))
}
train<-function(dataset_train){
library(MASS)
fit_lda <- lda(train.label~.,data=dataset_train)
return(fit_lda)
}
#####test
test<-function(fit_lda,dataset_test){
pred<-predict(fit_lda,newdata=dataset_test)
return(pred$class)
}
k_folds<-10
#the pca features number
k<-100
set.seed(1)
s <- sample(rep(1:10, c(rep(200, 10-1), 2000-(10-1)*200)))
cv.error <- rep(NA, k_folds)
for(i in 1:k_folds)
{
load(paste('rgb_train_',as.character(i),'.RData',sep=''))
load(paste('rgb_test_',as.character(i),'.RData',sep=''))
train.data<-t(P_train[1:k,])
test.data<-t(P_test[1:k,])
train.label <- label[s != i]
test.label <- label[s == i]
train.data=data.frame(train.data,train.label=as.factor(train.label))
fit <- train(train.data)
colnames(test.data)<-colnames(train.data)[1:k]
pred <- test(fit,as.data.frame(test.data))
test.label=as.factor(test.label)
cv.error[i] <- mean(pred != test.label)
print(paste(as.character(i/k_folds*100),'%',' completed',sep=''))
}
