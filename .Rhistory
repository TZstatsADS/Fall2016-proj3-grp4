library("forecast")
library("DT")
install.packages("DT")
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test_1009/original')
library("DT")
install.packages("DT")
library("DT")
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/app/test_yueqi')
data(mtcars)
data(mtcars)
mtcars
class(mtcars)
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
shiny::runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
runApp('C:/Study/Columbia/W4243_Applied_Data_Science/Project2/Fall2016-Proj2-grp6/app_new')
n<-200
K<-10
n.fold <- floor(n/K)
s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))
rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold))
help(sample)
rm(list = ls())
pixel_feature<-readRDS('image_matrix.rds')
pixel_feature<-readRDS('image_matrix.rds')
rm(list = ls())
label[1:1000]<-0
train<-function(dataset_train){
library(e1071)
tune.out=tune(svm ,train.label~.,data=dataset_train ,kernel ='linear',
ranges =list(cost=c(0.01, 0.1,1)))
bestmod =tune.out$best.model
return(bestmod)
}
#####test
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
test<-function(bestmod,dataset_test){
pred<-predict(bestmod,newdata=dataset_test)
return(pred)
}
source("http://bioconductor.org/biocLite.R")
biocLite()
source("http://bioconductor.org/biocLite.R")
biocLite()
library("EBImage")
gtk-demo
'gtk-demo'
gtk-demo
source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite("EBImage")
library("EBImage")
library("EBImage")
rm(list = ls())
feature <- function(sift_dir,sift_csv_name,img_dir,output_dir){
### Construct process features for training/testing images
### baseline feature: pca processed sift feature
### advanced feature: grb frequency feature
### Input: a csv file contain sift feature and a directory that contains images ready for processing
### Output: an .RData file contains processed features for the images
####################################################################################################
#1.process sift feature
#read sift feature csv
setwd(sift_dir)
sift_feature<-read.csv(sift_csv_name)
sift_feature<-t(sift_feature)
#Use the pca to process the sift feature
Sigma_sift=cov(sift_feature)
Sigma_eigen_sift=eigen(Sigma_sift)
Gamma_sift=Sigma_eigen_sift$vectors
P_sift=t(Gamma_sift)%*%t(sift_feature)
#choose the first k feature as output,we can draw the picture of the explained variance of the first k parameters and choose a good one
k<-20
PCA_sift<-t(P_sift[1:k,])
print('SIFT feature has been processed!')
####################################################################################################
####################################################################################################
#2.processed advanced feature
### load libraries
library("EBImage")
#set the img dir
setwd(img_dir)
#read the number of images
img_num <- length(list.files(img_dir))
#get the img list
list_images <- dir()
#set RGB color
nR <- 10
nG <- 8
nB <- 10
rgb_feature <- array(0, dim = c(img_num, nR*nG*nB))
#process images
for (i in 1:img_num){
img <- readImage(list_images[i])
mat <- imageData(img)
# Caution: the bins should be consistent across all images!
rBin <- seq(0, 1, length.out=nR)
gBin <- seq(0, 1, length.out=nG)
bBin <- seq(0, 1, length.out=nB)
freq_rgb <- as.data.frame(table(factor(findInterval(mat[,,1], rBin), levels=1:nR),
factor(findInterval(mat[,,2], gBin), levels=1:nG),
factor(findInterval(mat[,,3], bBin), levels=1:nB)))
rgb_feature[i,] <- as.numeric(freq_rgb$Freq)/(ncol(mat)*nrow(mat)) # normalization
print(paste(as.character(i/img_num*100),'%',' completed',sep=''))
}
#Using PCA to process rgb_feature to speed up train
Sigma_rgb=cov(rgb_feature)
Sigma_eigen_rgb=eigen(Sigma_rgb)
Gamma_rgb=Sigma_eigen_rgb$vectors
P_rgb=t(Gamma_rgb)%*%t(rgb_feature)
#choose the first k feature as output,we can draw the picture of the explained variance of the first k parameters and choose a good one
k<-100
PCA_rgb<-t(P_rgb[1:k,])
print('Advanced feature has been processed!')
####################################################################################################
### output constructed features
setwd(output_dir)
save(PCA_sift, file='feature_eval_baseline.RData')
save(PCA_rgb,  file='feature_eval_advanced.RData')
### determine img dimensions
#img0 <-  readImage(paste0(img_dir, img_name, "_", 1, ".jpg"))
#mat1 <- as.matrix(img0)
#n_r <- nrow(img0)
#n_c <- ncol(img0)
### store vectorized pixel values of images
#dat <- array(dim=c(n_files, n_r*n_c))
#for(i in 1:n_files){
#  img <- readImage(paste0(img_dir, img_name, "_", i, ".jpg"))
#  dat[i,] <- as.vector(img)
#}
### output constructed features
#if(!is.null(data_name)){
#  save(dat, file=paste0("./output/feature_eval", data_name, ".RData"))
#}
return(PCA_rgb)
}
feature('C:/Study/Columbia/W4243_Applied_Data_Science/Project3',
'sift_features.csv',
'C:/Study/Columbia/W4243_Applied_Data_Science/Project3/images',
'C:/Study/Columbia/W4243_Applied_Data_Science/Project3')
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3")
load('feature_eval_baseline.RData')
load('feature_eval_advanced.RData')
result=list(baseline=PCA_sift,adv=PCA_rgb)
feature_eval=list(baseline=PCA_sift,adv=PCA_rgb)
save(feature_eval, file='feature_eval.RData')
library(randomForest)
help(randomForest)
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4/data")
label<-rep(1,2000)
label[1:1000]<-0
label<-rep(0,2000)
label[1:1000]<-1
train<-function(dataset_train){
library(randomForest)
fit_rf <- randomForest(train.label~.,data=dataset_train,ntree=1500,type="classification",importance=TRUE)
return(fit_rf)
}
#####test
test<-function(fit_rf,dataset_test){
pred<-predict(fit_rf,newdata=dataset_test)
return(pred)
}
k_folds<-10
k<-100
set.seed(1)
s <- sample(rep(1:10, c(rep(200, 10-1), 2000-(10-1)*200)))
cv.error <- rep(NA, k_folds)
for(i in 1:k_folds)
{
load(paste('rgb_train_',as.character(i),'.RData',sep=''))
load(paste('rgb_test_',as.character(i),'.RData',sep=''))
train.data<-t(P_train[1:k,])
test.data<-t(P_test[1:k,])
train.label <- label[s != i]
test.label <- label[s == i]
train.data=data.frame(train.data,train.label=as.factor(train.label))
fit <- train(train.data)
colnames(test.data)<-colnames(train.data)[1:k]
pred <- test(fit,test.data)
test.label=as.factor(test.label)
cv.error[i] <- mean(pred != test.label)
print(paste(as.character(i/k_folds*100),'%',' completed',sep=''))
}
result<-c(mean(cv.error),sd(cv.error))
result
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3")
label<-rep(0,2000)
label[1:1000]<-1
load('feature_eval.RData')
train <- function(data_train,label_train){
### Train a Gradient Boosting Model (GBM) using PCA processed sift features from training images
### Also train a random forest model using PCA processed rgb data
### Input:
###  -  PCA processed sift features from images and PCA processed rgb data
###  -  class labels for training images
### Output: Two training models: baseline and adv
####################################################################################################
#1.train baseline model
### load libraries
library("gbm")
### Train with gradient boosting model
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=data_train$baseline, y=label_train,
n.trees=2000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=TRUE)
best_iter <- gbm.perf(fit_gbm, method="OOB")
####################################################################################################
####################################################################################################
#2.train adv model
### load libraries
library(randomForest)
### Train with random forest model
dataset_train=data.frame(data_train$adv,train.label=as.factor(label_train))
fit_rf <- randomForest(train.label~.,data=dataset_train,ntree=1500,type="classification",importance=TRUE)
####################################################################################################
return(list(gbm_fit=fit_gbm, gbm_iter=best_iter,rf_fit=fit_rf))
}
train <- function(data_train,label_train,par=NULL){
### Train a Gradient Boosting Model (GBM) using PCA processed sift features from training images
### Also train a random forest model using PCA processed rgb data
### Input:
###  -  PCA processed sift features from images and PCA processed rgb data
###  -  class labels for training images
### Output: Two training models: baseline and adv
####################################################################################################
#1.train baseline model
### load libraries
library("gbm")
### Train with gradient boosting model
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=data_train$baseline, y=label_train,
n.trees=2000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=TRUE)
best_iter <- gbm.perf(fit_gbm, method="OOB")
####################################################################################################
####################################################################################################
#2.train adv model
### load libraries
library(randomForest)
### Train with random forest model
dataset_train=data.frame(data_train$adv,train.label=as.factor(label_train))
fit_rf <- randomForest(train.label~.,data=dataset_train,ntree=1500,type="classification",importance=TRUE)
####################################################################################################
return(list(gbm_fit=fit_gbm, gbm_iter=best_iter,rf_fit=fit_rf))
}
train_model<-train(feature_eval,label_train)
train_model<-train(feature_eval,label)
test <- function(fit_train, dat_test){
### Fit the classfication model with testing data
### Input:
###  - the fitted classification model using training data
###  -  processed features from testing images
### Output: training model specification
####################################################################################################
#1.fit the baseline model
### load libraries
library("gbm")
pred_gbm <- predict(fit_train$gbm_fit, newdata=dat_test,
n.trees=fit_train$gbm_iter, type="response")
####################################################################################################
####################################################################################################
#2.fit the advanced model
### load libraries
library(randomForest)
pred_rf<-predict(fit_train$rf_fit,newdata=dat_test)
####################################################################################################
return(list(baseline=as.numeric(pred_gbm> 0.5),adv=pred_rf))
}
test <- function(fit_train, dat_test_baseline,dat_test_adv){
### Fit the classfication model with testing data
### Input:
###  - the fitted classification model using training data
###  -  processed features from testing images
### Output: training model specification
####################################################################################################
#1.fit the baseline model
### load libraries
library("gbm")
pred_gbm <- predict(fit_train$gbm_fit, newdata=dat_test_baseline,
n.trees=fit_train$gbm_iter, type="response")
####################################################################################################
####################################################################################################
#2.fit the advanced model
### load libraries
library(randomForest)
pred_rf<-predict(fit_train$rf_fit,newdata=dat_test_adv)
####################################################################################################
return(list(baseline=as.numeric(pred_gbm> 0.5),adv=pred_rf))
}
test(train_model,feature_eval$baseline,feature_eval$adv)
test(train_model,feature_eval$baseline,feature_eval$adv)
colnames(feature_eval$adv)
feature_eval$adv
test <- function(fit_train, dat_test_baseline,dat_test_adv){
### Fit the classfication model with testing data
### Input:
###  - the fitted classification model using training data
###  -  processed features from testing images
### Output: training model specification
####################################################################################################
#1.fit the baseline model
### load libraries
library("gbm")
pred_gbm <- predict(fit_train$gbm_fit, newdata=dat_test_baseline,
n.trees=fit_train$gbm_iter, type="response")
####################################################################################################
####################################################################################################
#2.fit the advanced model
### load libraries
library(randomForest)
pred_rf<-predict(fit_train$rf_fit,newdata=data.frame(dat_test_adv))
####################################################################################################
return(list(baseline=as.numeric(pred_gbm> 0.5),adv=pred_rf))
}
test(train_model,feature_eval$baseline,feature_eval$adv)
result<-test(train_model,feature_eval$baseline,feature_eval$adv)
result$baseline
cv.error <- mean(result$baseline != label)
cv.error <- mean(result$adv != label)
result$adv
cv.error <- mean(result$adv[998:1020] != label)
result$adv
length(result$adv)
result$adv[999:1001]
result$adv
cv.error <- mean(result$adv != label)
cv.error <- mean(result$baseline != label)
save(train_model,file='train_model.RData')
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4")
load("./output/feature_eval.RData")
label<-rep(0,2000)
label[1:1000]<-1
save(label,file='label_eval.RData')
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4")
load("./output/feature_eval.RData")
load("./data/label_eval.RData")
source("./lib/train.R")
source("./lib/test.R")
n <- 2000
n_rep <- 20
K <- 5
label_eval<-rep(0,2000)
label_eval[1:1000]<-1
save(label_eval,file='label_eval.RData')
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4")
load("./output/feature_eval.RData")
load("./data/label_eval.RData")
source("./lib/train.R")
source("./lib/test.R")
n <- 2000
n_rep <- 20
K <- 5
ind_cat <- which(label_eval == 1) # 1000 cats
ind_dog <- which(label_eval == 0) # 1000 dogs
n_cat_fold <- n_dog_fold <- 200
CV_err_baseline <- rep(0, n_rep)
CV_err_adv <- rep(0, n_rep)
CV_fit_baseline <- array(dim=c(n, n_rep))
CV_fit_adv <- array(dim=c(n, n_rep))
train_time <- array(dim=c(K, n_rep))
source("./lib/train.R")
source("./lib/test.R")
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4")
load("./output/feature_eval.RData")
load("./data/label_eval.RData")
source("./lib/train.R")
source("./lib/test.R")
n <- 2000
n_rep <- 20
K <- 5
ind_cat <- which(label_eval == 1) # 1000 cats
ind_dog <- which(label_eval == 0) # 1000 dogs
n_cat_fold <- n_dog_fold <- 200
CV_err_baseline <- rep(0, n_rep)
CV_err_adv <- rep(0, n_rep)
CV_fit_baseline <- array(dim=c(n, n_rep))
CV_fit_adv <- array(dim=c(n, n_rep))
train_time <- array(dim=c(K, n_rep))
for(r in 1:n_rep){
set.seed(309+r)
assign_cat <- sample(rep(1:K, times=n_cat_fold))
set.seed(1310+r)
assign_dog <- sample(rep(1:K, times=n_dog_fold))
CV_index <- rep(NA, n)
CV_index[ind_cat] <- assign_cat
CV_index[ind_dog] <- assign_dog
for(c in 1:K){
cat("fold= ", c, "\n")
ind_test <- which(CV_index == c)
dat_train_baseline <- feature_eval$baseline[-ind_test,]
dat_train_adv <- feature_eval$adv[-ind_test,]
label_train <- label_eval[-ind_test]
dat_test_baseline <- feature_eval$baseline[ind_test,]
dat_test_adv <- feature_eval$adv[ind_test,]
label_test <- label_eval[ind_test]
train_time[c,r] <- system.time(mod_train <- train(dat_train_baseline,dat_train_adv,label_train))[1]
pred_test <- test(mod_train, dat_test_baseline,dat_test_adv)
CV_fit_baseline[ind_test, r] <- pred_test$baseline
CV_fit_adv[ind_test, r] <- pred_test$adv
}
CV_err_baseline[r] <- mean(CV_fit_baseline[,r] != label_eval)
CV_err_adv[r] <- mean(CV_fit_adv[,r] != label_eval)
print(r)
}
save(CV_fit_baseline, CV_fit_adv,  CV_err_baseline, CV_err_adv, train_time, file="CV_result.RData")
cat("Mean of Baseline CV Error =", round(mean(CV_err_baseline), digits=4), "\n")
cat("SD of Baseline CV Error =", round(sd(CV_err_baseline), digits=4), "\n")
cat("Mean of Adv CV Error =", round(mean(CV_err_adv), digits=4), "\n")
cat("SD of Adv CV Error =", round(sd(CV_err_adv), digits=4), "\n")
cat("Mean Training Name =", round(mean(train_time), digits=2), "\n")
pred_test$adv
save(CV_fit_baseline, CV_fit_adv,  CV_err_baseline, CV_err_adv, train_time, file="CV_result.RData")
train_time
label_eval
(CV_fit_adv[,r]
CV_fit_adv[,r]
View(CV_fit_adv)
View(CV_fit_adv)
View(dat_test_adv)
predict(mod_train$rf_fit,newdata=data.frame(dat_test_adv))
pred_test$adv
pred_test$adv
CV_fit_adv[ind_test, r]
CV_fit_adv[ind_test, r] <- pred_test$adv
CV_fit_adv[ind_test, r]
pred_test$adv
CV_fit_baseline[ind_test, r]
pred_test$baseline
CV_fit_adv[ind_test, r] <- as.numeric(pred_test$adv)
CV_fit_adv[ind_test, r]
as.numeric(paste(pred_test$adv))
CV_fit_adv[ind_test, r] <- as.numeric(paste(pred_test$adv))
CV_fit_adv[ind_test, r]
cat("Mean of Baseline CV Error =", round(mean(CV_err_baseline), digits=4), "\n")
rm(list = ls())
setwd("C:/Study/Columbia/W4243_Applied_Data_Science/Project3/Fall2016-proj3-grp4")
load("./output/feature_eval.RData")
load("./data/label_eval.RData")
source("./lib/train.R")
source("./lib/test.R")
n <- 2000
n_rep <- 20
K <- 5
ind_cat <- which(label_eval == 1) # 1000 cats
ind_dog <- which(label_eval == 0) # 1000 dogs
n_cat_fold <- n_dog_fold <- 200
CV_err_baseline <- rep(0, n_rep)
CV_err_adv <- rep(0, n_rep)
CV_fit_baseline <- array(dim=c(n, n_rep))
CV_fit_adv <- array(dim=c(n, n_rep))
train_time <- array(dim=c(K, n_rep))
for(r in 1:n_rep){
set.seed(309+r)
assign_cat <- sample(rep(1:K, times=n_cat_fold))
set.seed(1310+r)
assign_dog <- sample(rep(1:K, times=n_dog_fold))
CV_index <- rep(NA, n)
CV_index[ind_cat] <- assign_cat
CV_index[ind_dog] <- assign_dog
for(c in 1:K){
cat("fold= ", c, "\n")
ind_test <- which(CV_index == c)
dat_train_baseline <- feature_eval$baseline[-ind_test,]
dat_train_adv <- feature_eval$adv[-ind_test,]
label_train <- label_eval[-ind_test]
dat_test_baseline <- feature_eval$baseline[ind_test,]
dat_test_adv <- feature_eval$adv[ind_test,]
label_test <- label_eval[ind_test]
train_time[c,r] <- system.time(mod_train <- train(dat_train_baseline,dat_train_adv,label_train))[1]
pred_test <- test(mod_train, dat_test_baseline,dat_test_adv)
CV_fit_baseline[ind_test, r] <- pred_test$baseline
CV_fit_adv[ind_test, r] <- pred_test$adv
}
CV_err_baseline[r] <- mean(CV_fit_baseline[,r] != label_eval)
CV_err_adv[r] <- mean(CV_fit_adv[,r] != label_eval)
print(r)
}
cat("Mean of Baseline CV Error =", round(mean(CV_err_baseline), digits=4), "\n")
cat("SD of Baseline CV Error =", round(sd(CV_err_baseline), digits=4), "\n")
cat("Mean of Adv CV Error =", round(mean(CV_err_adv), digits=4), "\n")
cat("SD of Adv CV Error =", round(sd(CV_err_adv), digits=4), "\n")
cat("Mean Training Name =", round(mean(train_time), digits=2), "\n")
save(CV_fit_baseline, CV_fit_adv,  CV_err_baseline, CV_err_adv, train_time, file="CV_result.RData")
